services:
  client:
    build: 
      context: ./client
      dockerfile: Dockerfile
    container_name: projector_frontend
    restart: unless-stopped
    ports:
      - '4000:3000'
    depends_on:
      - projector-backend
    environment:
      - NEXT_PUBLIC_API_URL=http://projector-backend:5000

  projector-backend:
    build:
      context: ./server  # This matches your actual folder name

      context: ./server

      dockerfile: Dockerfile
    container_name: projector_nest
    restart: unless-stopped
    ports:
      - '5000:3000'
    environment:
      - NODE_ENV=production
      - AI_SERVICE_URL=http://ai-model:5000/generate
      - LOG_LEVEL=simple
    volumes:
      - ./server:/app
      - /app/node_modules

  ai-model:
    platform: linux/amd64
    image: ghcr.io/huggingface/text-generation-inference:1.1.0
    container_name: projector_ai
    restart: unless-stopped
    ports:
      - '5001:5000'
    environment:
      - MODEL_ID=TinyLlama/TinyLlama-1.1B-Chat-v1.0
      - QUANTIZE=bitsandbytes-nf4
      - MAX_INPUT_LENGTH=1024
      - MAX_TOTAL_TOKENS=2048
    volumes:
      - ai-model-cache:/data
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G

  chromium:
    build:
      context: ./server
      dockerfile: Dockerfile.chromium
    container_name: projector_chromium
    depends_on:
      - projector-backend
    restart: unless-stopped
    environment:
      - DISPLAY=:99

volumes:
  ai-model-cache:
